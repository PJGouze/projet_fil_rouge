{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8af87a",
   "metadata": {},
   "source": [
    "# MLP Regressor\n",
    "\n",
    "**Objective**: Train multi-output regressors on text embeddings + explanatory variables.\n",
    "- Scenario 1: IID data with random split (60/20/20)\n",
    "- Scenario 2: IID train/val, OOD test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd6fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x709a21934110>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad264677",
   "metadata": {},
   "source": [
    "## 1. Load Data from Previous Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b427e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded\n",
      "  - df_merged shape: (6574, 28)\n",
      "  - embeddings_pca_optimal shape: (6574, 43)\n",
      "  - IID samples: 6352\n",
      "  - OOD samples: 222\n",
      "\n",
      "✓ Features prepared: (6574, 53)\n",
      "✓ Targets prepared: (6574, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load merged data and embeddings\n",
    "df_merged = pd.read_csv('data_merged.csv')\n",
    "embeddings_pca_optimal = np.load('embeddings_pca_optimal.npy')\n",
    "\n",
    "print(f\"✓ Data loaded\")\n",
    "print(f\"  - df_merged shape: {df_merged.shape}\")\n",
    "print(f\"  - embeddings_pca_optimal shape: {embeddings_pca_optimal.shape}\")\n",
    "print(f\"  - IID samples: {(df_merged['OOD'] == 0).sum()}\")\n",
    "print(f\"  - OOD samples: {(df_merged['OOD'] == 1).sum()}\")\n",
    "\n",
    "# Define column lists (same as pipeline)\n",
    "vars_expl = [\n",
    "    \"MS % brut\", \"PB % brut\", \"CB % brut\", \"MGR % brut\", \"MM % brut\",\n",
    "    \"NDF % brut\", \"ADF % brut\", \"Lignine % brut\", \"Amidon % brut\", \"Sucres % brut\"\n",
    "]\n",
    "\n",
    "vars_cibles = [\n",
    "    \"EB (kcal) kcal/kg brut\", \"ED porc croissance (kcal) kcal/kg brut\", \"EM porc croissance (kcal) kcal/kg brut\",\n",
    "    \"EN porc croissance (kcal) kcal/kg brut\", \"EMAn coq (kcal) kcal/kg brut\", \"EMAn poulet (kcal) kcal/kg brut\",\n",
    "    \"UFL 2018 par kg brut\", \"UFV 2018 par kg brut\", \"PDIA 2018 g/kg brut\", \"PDI 2018 g/kg brut\", \"BalProRu 2018 g/kg brut\"\n",
    "]\n",
    "\n",
    "# Prepare features\n",
    "X_embeddings = embeddings_pca_optimal  # (6574, 43)\n",
    "X_vars = df_merged[vars_expl].fillna(0).values\n",
    "X_combined = np.hstack([X_embeddings, X_vars])  # (6574, 53)\n",
    "\n",
    "# Prepare targets\n",
    "y_combined = df_merged[vars_cibles].fillna(0).values  # (6574, 11)\n",
    "\n",
    "print(f\"\\n✓ Features prepared: {X_combined.shape}\")\n",
    "print(f\"✓ Targets prepared: {y_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd1449",
   "metadata": {},
   "source": [
    "## 2. Define PyTorch MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc3d668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model architecture:\n",
      "MLPRegressor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=53, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=64, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "✓ Total parameters: 55,691\n"
     ]
    }
   ],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.2):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Test model initialization\n",
    "model_test = MLPRegressor(input_size=53, hidden_sizes=[256, 128, 64], output_size=11, dropout_rate=0.2)\n",
    "print(f\"✓ Model architecture:\")\n",
    "print(model_test)\n",
    "print(f\"\\n✓ Total parameters: {sum(p.numel() for p in model_test.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675e462",
   "metadata": {},
   "source": [
    "## 3. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6a60438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training utilities defined\n"
     ]
    }
   ],
   "source": [
    "def create_dataloaders(X, y, batch_size=32, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    \"\"\"Create train/val/test dataloaders from data\"\"\"\n",
    "    n_samples = len(X)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    # Split indices\n",
    "    train_size = int(n_samples * train_ratio)\n",
    "    val_size = int(n_samples * val_ratio)\n",
    "    \n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size:train_size + val_size]\n",
    "    test_idx = indices[train_size + val_size:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X[train_idx]), torch.FloatTensor(y[train_idx])\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X[val_idx]), torch.FloatTensor(y[val_idx])\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X[test_idx]), torch.FloatTensor(y[test_idx])\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, (X[train_idx], y[train_idx]), (X[val_idx], y[val_idx]), (X[test_idx], y[test_idx])\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def get_predictions(model, X, device, batch_size=32):\n",
    "    \"\"\"Get predictions for dataset\"\"\"\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(torch.FloatTensor(X))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    return np.vstack(predictions)\n",
    "\n",
    "print(\"✓ Training utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a65541",
   "metadata": {},
   "source": [
    "## 4. Scenario 1: IID Data with Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ed82f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 1: In-Distribution (IID) Data with Random Split (60/20/20)\n",
      "======================================================================\n",
      "\n",
      "✓ IID dataset: 6352 samples\n",
      "  - Train: 3811 samples\n",
      "  - Val:   1270 samples\n",
      "  - Test:  1271 samples\n",
      "\n",
      "Training MLP (max 500 epochs, patience 50)...\n",
      "  Epoch  10: Train Loss=0.0652, Val Loss=0.0256\n",
      "  Epoch  20: Train Loss=0.0571, Val Loss=0.0232\n",
      "  Epoch  30: Train Loss=0.0538, Val Loss=0.0122\n",
      "  Epoch  40: Train Loss=0.0506, Val Loss=0.0168\n",
      "  Epoch  50: Train Loss=0.0543, Val Loss=0.0134\n",
      "  Epoch  60: Train Loss=0.0495, Val Loss=0.0130\n",
      "  Epoch  70: Train Loss=0.0458, Val Loss=0.0142\n",
      "  Epoch  80: Train Loss=0.0469, Val Loss=0.0104\n",
      "  Epoch  90: Train Loss=0.0456, Val Loss=0.0148\n",
      "  Epoch 100: Train Loss=0.0448, Val Loss=0.0178\n",
      "  Epoch 110: Train Loss=0.0435, Val Loss=0.0113\n",
      "  Epoch 120: Train Loss=0.0431, Val Loss=0.0115\n",
      "  Epoch 130: Train Loss=0.0426, Val Loss=0.0115\n",
      "  Epoch 140: Train Loss=0.0441, Val Loss=0.0185\n",
      "  Epoch 150: Train Loss=0.0429, Val Loss=0.0102\n",
      "  Epoch 160: Train Loss=0.0411, Val Loss=0.0098\n",
      "  Epoch 170: Train Loss=0.0444, Val Loss=0.0102\n",
      "  Epoch 180: Train Loss=0.0431, Val Loss=0.0092\n",
      "  Epoch 190: Train Loss=0.0431, Val Loss=0.0120\n",
      "  Epoch 200: Train Loss=0.0438, Val Loss=0.0109\n",
      "\n",
      "Early stopping at epoch 203\n",
      "✓ Training complete (epoch 203)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCENARIO 1: In-Distribution (IID) Data with Random Split (60/20/20)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filter IID data only\n",
    "iid_mask = df_merged['OOD'] == 0\n",
    "X_iid = X_combined[iid_mask]\n",
    "y_iid = y_combined[iid_mask]\n",
    "\n",
    "print(f\"\\n✓ IID dataset: {X_iid.shape[0]} samples\")\n",
    "\n",
    "# Split FIRST (60/20/20) to avoid data leakage\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_iid, y_iid, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Fit StandardScaler ONLY on training data (for features)\n",
    "scaler_X_s1 = StandardScaler()\n",
    "X_train_s1 = scaler_X_s1.fit_transform(X_train)\n",
    "X_val_s1 = scaler_X_s1.transform(X_val)\n",
    "X_test_s1 = scaler_X_s1.transform(X_test)\n",
    "\n",
    "# Fit StandardScaler ONLY on training data (for targets)\n",
    "scaler_y_s1 = StandardScaler()\n",
    "y_train_s1_scaled = scaler_y_s1.fit_transform(y_train)\n",
    "y_val_s1_scaled = scaler_y_s1.transform(y_val)\n",
    "y_test_s1_scaled = scaler_y_s1.transform(y_test)\n",
    "\n",
    "# Keep original targets for evaluation\n",
    "y_train_s1 = y_train\n",
    "y_val_s1 = y_val\n",
    "y_test_s1 = y_test\n",
    "\n",
    "print(f\"  - Train: {len(X_train_s1)} samples\")\n",
    "print(f\"  - Val:   {len(X_val_s1)} samples\")\n",
    "print(f\"  - Test:  {len(X_test_s1)} samples\")\n",
    "\n",
    "# Create dataloaders (using scaled targets)\n",
    "train_dataset_s1 = TensorDataset(torch.FloatTensor(X_train_s1), torch.FloatTensor(y_train_s1_scaled))\n",
    "val_dataset_s1 = TensorDataset(torch.FloatTensor(X_val_s1), torch.FloatTensor(y_val_s1_scaled))\n",
    "test_dataset_s1 = TensorDataset(torch.FloatTensor(X_test_s1), torch.FloatTensor(y_test_s1_scaled))\n",
    "\n",
    "train_loader_s1 = DataLoader(train_dataset_s1, batch_size=32, shuffle=True)\n",
    "val_loader_s1 = DataLoader(val_dataset_s1, batch_size=32, shuffle=False)\n",
    "test_loader_s1 = DataLoader(test_dataset_s1, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model_s1 = MLPRegressor(input_size=53, hidden_sizes=[256, 128, 64], output_size=11, dropout_rate=0.2).to(device)\n",
    "optimizer_s1 = optim.Adam(model_s1.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\nTraining MLP (max {num_epochs} epochs, patience {patience})...\")\n",
    "\n",
    "train_losses_s1 = []\n",
    "val_losses_s1 = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model_s1, train_loader_s1, optimizer_s1, criterion, device)\n",
    "    val_loss = evaluate(model_s1, val_loader_s1, criterion, device)\n",
    "    \n",
    "    train_losses_s1.append(train_loss)\n",
    "    val_losses_s1.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_s1 = model_s1.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            model_s1.load_state_dict(best_model_s1)\n",
    "            break\n",
    "\n",
    "print(f\"✓ Training complete (epoch {epoch+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f2aa6",
   "metadata": {},
   "source": [
    "### Scenario 1: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165e54c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 1 - Performance per target variable:\n",
      "Target                              R² (test)       RMSE            MAE            \n",
      "--------------------------------------------------------------------------------\n",
      "EB (kcal) kcal/kg brut               0.989             65.83            51.54\n",
      "ED porc croissance (kcal) kcal/kg    0.986            105.22            73.87\n",
      "EM porc croissance (kcal) kcal/kg    0.986             98.27            68.13\n",
      "EN porc croissance (kcal) kcal/kg    0.989             78.30            59.78\n",
      "EMAn coq (kcal) kcal/kg brut         0.989             88.61            68.58\n",
      "EMAn poulet (kcal) kcal/kg brut      0.989             89.23            67.44\n",
      "UFL 2018 par kg brut                 0.987              0.03             0.02\n",
      "UFV 2018 par kg brut                 0.986              0.04             0.03\n",
      "PDIA 2018 g/kg brut                  0.990              7.99             4.94\n",
      "PDI 2018 g/kg brut                   0.989              8.26             5.25\n",
      "BalProRu 2018 g/kg brut              0.984             12.23             9.34\n",
      "\n",
      "Overall Statistics (Scenario 1):\n",
      "  - Mean R² (test):   0.988 ± 0.002\n",
      "  - Mean RMSE (test): 50.37 ± 44.03\n",
      "  - Mean MAE (test):  37.17 ± 32.43\n"
     ]
    }
   ],
   "source": [
    "# Get predictions (in scaled space)\n",
    "y_train_pred_s1_scaled = get_predictions(model_s1, X_train_s1, device)\n",
    "y_val_pred_s1_scaled = get_predictions(model_s1, X_val_s1, device)\n",
    "y_test_pred_s1_scaled = get_predictions(model_s1, X_test_s1, device)\n",
    "\n",
    "# Inverse-transform predictions to original scale\n",
    "y_train_pred_s1 = scaler_y_s1.inverse_transform(y_train_pred_s1_scaled)\n",
    "y_val_pred_s1 = scaler_y_s1.inverse_transform(y_val_pred_s1_scaled)\n",
    "y_test_pred_s1 = scaler_y_s1.inverse_transform(y_test_pred_s1_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"\\nScenario 1 - Performance per target variable:\")\n",
    "print(f\"{'Target':<35} {'R² (test)':<15} {'RMSE':<15} {'MAE':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results_s1 = []\n",
    "for i, target in enumerate(vars_cibles):\n",
    "    r2_test = r2_score(y_test_s1[:, i], y_test_pred_s1[:, i])\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_s1[:, i], y_test_pred_s1[:, i]))\n",
    "    mae_test = mean_absolute_error(y_test_s1[:, i], y_test_pred_s1[:, i])\n",
    "    \n",
    "    results_s1.append({\n",
    "        'target': target,\n",
    "        'R2_test': r2_test,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'MAE_test': mae_test\n",
    "    })\n",
    "    \n",
    "    print(f\"{target[:33]:<35} {r2_test:>6.3f}        {rmse_test:>10.2f}       {mae_test:>10.2f}\")\n",
    "\n",
    "df_results_s1 = pd.DataFrame(results_s1)\n",
    "print(f\"\\nOverall Statistics (Scenario 1):\")\n",
    "print(f\"  - Mean R² (test):   {df_results_s1['R2_test'].mean():.3f} ± {df_results_s1['R2_test'].std():.3f}\")\n",
    "print(f\"  - Mean RMSE (test): {df_results_s1['RMSE_test'].mean():.2f} ± {df_results_s1['RMSE_test'].std():.2f}\")\n",
    "print(f\"  - Mean MAE (test):  {df_results_s1['MAE_test'].mean():.2f} ± {df_results_s1['MAE_test'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ce0e3",
   "metadata": {},
   "source": [
    "## 5. Scenario 2: IID Train/Val, OOD Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d1b995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 2: IID Train/Val, OOD Test (Out-of-Distribution Detection)\n",
      "======================================================================\n",
      "\n",
      "✓ Data split:\n",
      "  - IID (train/val): 6352 samples\n",
      "  - OOD (test):      222 samples\n",
      "  - Train: 4764 samples\n",
      "  - Val:   1588 samples\n",
      "  - Test (OOD): 222 samples\n",
      "\n",
      "Training MLP (max 500 epochs, patience 50)...\n",
      "  Epoch  10: Train Loss=0.0630, Val Loss=0.0197\n",
      "  Epoch  20: Train Loss=0.0522, Val Loss=0.0136\n",
      "  Epoch  30: Train Loss=0.0499, Val Loss=0.0169\n",
      "  Epoch  40: Train Loss=0.0505, Val Loss=0.0130\n",
      "  Epoch  50: Train Loss=0.0477, Val Loss=0.0137\n",
      "  Epoch  60: Train Loss=0.0450, Val Loss=0.0128\n",
      "  Epoch  70: Train Loss=0.0453, Val Loss=0.0106\n",
      "  Epoch  80: Train Loss=0.0478, Val Loss=0.0116\n",
      "  Epoch  90: Train Loss=0.0469, Val Loss=0.0154\n",
      "  Epoch 100: Train Loss=0.0452, Val Loss=0.0106\n",
      "  Epoch 110: Train Loss=0.0432, Val Loss=0.0111\n",
      "  Epoch 120: Train Loss=0.0450, Val Loss=0.0146\n",
      "  Epoch 130: Train Loss=0.0441, Val Loss=0.0079\n",
      "  Epoch 140: Train Loss=0.0429, Val Loss=0.0111\n",
      "  Epoch 150: Train Loss=0.0433, Val Loss=0.0108\n",
      "  Epoch 160: Train Loss=0.0417, Val Loss=0.0106\n",
      "  Epoch 170: Train Loss=0.0421, Val Loss=0.0111\n",
      "\n",
      "Early stopping at epoch 178\n",
      "✓ Training complete (epoch 178)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCENARIO 2: IID Train/Val, OOD Test (Out-of-Distribution Detection)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data\n",
    "iid_mask = df_merged['OOD'] == 0\n",
    "ood_mask = df_merged['OOD'] == 1\n",
    "\n",
    "X_iid_s2 = X_combined[iid_mask]\n",
    "y_iid_s2 = y_combined[iid_mask]\n",
    "\n",
    "X_ood_s2 = X_combined[ood_mask]\n",
    "y_ood_s2 = y_combined[ood_mask]\n",
    "\n",
    "print(f\"\\n✓ Data split:\")\n",
    "print(f\"  - IID (train/val): {X_iid_s2.shape[0]} samples\")\n",
    "print(f\"  - OOD (test):      {X_ood_s2.shape[0]} samples\")\n",
    "\n",
    "# Split IID into train/val FIRST (75/25) to avoid data leakage\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_s2, X_val_s2, y_train_s2, y_val_s2 = train_test_split(\n",
    "    X_iid_s2, y_iid_s2, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Fit StandardScaler ONLY on training data (for features)\n",
    "scaler_X_s2 = StandardScaler()\n",
    "X_train_s2 = scaler_X_s2.fit_transform(X_train_s2)\n",
    "X_val_s2 = scaler_X_s2.transform(X_val_s2)\n",
    "X_ood_scaled_s2 = scaler_X_s2.transform(X_ood_s2)\n",
    "\n",
    "# Fit StandardScaler ONLY on training data (for targets)\n",
    "scaler_y_s2 = StandardScaler()\n",
    "y_train_s2_scaled = scaler_y_s2.fit_transform(y_train_s2)\n",
    "y_val_s2_scaled = scaler_y_s2.transform(y_val_s2)\n",
    "y_ood_s2_scaled = scaler_y_s2.transform(y_ood_s2)\n",
    "\n",
    "print(f\"  - Train: {len(X_train_s2)} samples\")\n",
    "print(f\"  - Val:   {len(X_val_s2)} samples\")\n",
    "print(f\"  - Test (OOD): {len(X_ood_scaled_s2)} samples\")\n",
    "\n",
    "# Create dataloaders (using scaled targets)\n",
    "train_dataset_s2 = TensorDataset(torch.FloatTensor(X_train_s2), torch.FloatTensor(y_train_s2_scaled))\n",
    "val_dataset_s2 = TensorDataset(torch.FloatTensor(X_val_s2), torch.FloatTensor(y_val_s2_scaled))\n",
    "test_dataset_s2 = TensorDataset(torch.FloatTensor(X_ood_scaled_s2), torch.FloatTensor(y_ood_s2_scaled))\n",
    "\n",
    "train_loader_s2 = DataLoader(train_dataset_s2, batch_size=32, shuffle=True)\n",
    "val_loader_s2 = DataLoader(val_dataset_s2, batch_size=32, shuffle=False)\n",
    "test_loader_s2 = DataLoader(test_dataset_s2, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model_s2 = MLPRegressor(input_size=53, hidden_sizes=[256, 128, 64], output_size=11, dropout_rate=0.2).to(device)\n",
    "optimizer_s2 = optim.Adam(model_s2.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nTraining MLP (max {num_epochs} epochs, patience {patience})...\")\n",
    "\n",
    "train_losses_s2 = []\n",
    "val_losses_s2 = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model_s2, train_loader_s2, optimizer_s2, criterion, device)\n",
    "    val_loss = evaluate(model_s2, val_loader_s2, criterion, device)\n",
    "    \n",
    "    train_losses_s2.append(train_loss)\n",
    "    val_losses_s2.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_s2 = model_s2.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            model_s2.load_state_dict(best_model_s2)\n",
    "            break\n",
    "\n",
    "print(f\"✓ Training complete (epoch {epoch+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfb24c",
   "metadata": {},
   "source": [
    "### Scenario 2: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fd2d3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 2: Evaluation (OOD generalization)\n",
      "======================================================================\n",
      "\n",
      "SCENARIO 2 Results:\n",
      "\n",
      "Train Set:\n",
      "  R²:   0.9911 (±0.0020)\n",
      "  RMSE: 41.5157 (±33.4487)\n",
      "  MAE:  31.1294 (±25.3044)\n",
      "\n",
      "Validation Set:\n",
      "  R²:   0.9903 (±0.0018)\n",
      "  RMSE: 43.1104 (±34.8587)\n",
      "  MAE:  32.5211 (±26.5978)\n",
      "\n",
      "Test Set (OOD - Feedtables):\n",
      "  R²:   0.7467 (±0.2615)\n",
      "  RMSE: 281.6015 (±243.7589)\n",
      "  MAE:  140.8519 (±124.8781)\n",
      "\n",
      "Per-variable R² (Test Set):\n",
      "  EB (kcal) kcal/kg brut: R² =  0.9666\n",
      "  ED porc croissance (kcal) kcal/kg brut: R² =  0.9336\n",
      "  EM porc croissance (kcal) kcal/kg brut: R² =  0.9366\n",
      "  EN porc croissance (kcal) kcal/kg brut: R² =  0.9526\n",
      "  EMAn coq (kcal) kcal/kg brut: R² =  0.8839\n",
      "  EMAn poulet (kcal) kcal/kg brut: R² =  0.8670\n",
      "  UFL 2018 par kg brut: R² =  0.8386\n",
      "  UFV 2018 par kg brut: R² =  0.8538\n",
      "  PDIA 2018 g/kg brut : R² =  0.3906\n",
      "  PDI 2018 g/kg brut  : R² =  0.3324\n",
      "  BalProRu 2018 g/kg brut: R² =  0.2579\n",
      "\n",
      "OOD Performance Drop: 24.6%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCENARIO 2: Evaluation (OOD generalization)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions in scaled space (pass arrays, not dataloaders)\n",
    "y_train_pred_s2_scaled = get_predictions(model_s2, X_train_s2, device)\n",
    "y_val_pred_s2_scaled = get_predictions(model_s2, X_val_s2, device)\n",
    "y_test_pred_s2_scaled = get_predictions(model_s2, X_ood_scaled_s2, device)\n",
    "\n",
    "# Inverse-transform predictions back to original scale\n",
    "y_train_pred_s2 = scaler_y_s2.inverse_transform(y_train_pred_s2_scaled)\n",
    "y_val_pred_s2 = scaler_y_s2.inverse_transform(y_val_pred_s2_scaled)\n",
    "y_test_pred_s2 = scaler_y_s2.inverse_transform(y_test_pred_s2_scaled)\n",
    "\n",
    "# Calculate metrics on original scale\n",
    "print(\"\\nSCENARIO 2 Results:\")\n",
    "print(\"\\nTrain Set:\")\n",
    "r2_train_s2 = r2_score(y_train_s2, y_train_pred_s2, multioutput='raw_values')\n",
    "rmse_train_s2 = np.sqrt(mean_squared_error(y_train_s2, y_train_pred_s2, multioutput='raw_values'))\n",
    "mae_train_s2 = mean_absolute_error(y_train_s2, y_train_pred_s2, multioutput='raw_values')\n",
    "\n",
    "print(f\"  R²:   {r2_train_s2.mean():.4f} (±{r2_train_s2.std():.4f})\")\n",
    "print(f\"  RMSE: {rmse_train_s2.mean():.4f} (±{rmse_train_s2.std():.4f})\")\n",
    "print(f\"  MAE:  {mae_train_s2.mean():.4f} (±{mae_train_s2.std():.4f})\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "r2_val_s2 = r2_score(y_val_s2, y_val_pred_s2, multioutput='raw_values')\n",
    "rmse_val_s2 = np.sqrt(mean_squared_error(y_val_s2, y_val_pred_s2, multioutput='raw_values'))\n",
    "mae_val_s2 = mean_absolute_error(y_val_s2, y_val_pred_s2, multioutput='raw_values')\n",
    "\n",
    "print(f\"  R²:   {r2_val_s2.mean():.4f} (±{r2_val_s2.std():.4f})\")\n",
    "print(f\"  RMSE: {rmse_val_s2.mean():.4f} (±{rmse_val_s2.std():.4f})\")\n",
    "print(f\"  MAE:  {mae_val_s2.mean():.4f} (±{mae_val_s2.std():.4f})\")\n",
    "\n",
    "print(\"\\nTest Set (OOD - Feedtables):\")\n",
    "r2_test_s2 = r2_score(y_ood_s2, y_test_pred_s2, multioutput='raw_values')\n",
    "rmse_test_s2 = np.sqrt(mean_squared_error(y_ood_s2, y_test_pred_s2, multioutput='raw_values'))\n",
    "mae_test_s2 = mean_absolute_error(y_ood_s2, y_test_pred_s2, multioutput='raw_values')\n",
    "\n",
    "print(f\"  R²:   {r2_test_s2.mean():.4f} (±{r2_test_s2.std():.4f})\")\n",
    "print(f\"  RMSE: {rmse_test_s2.mean():.4f} (±{rmse_test_s2.std():.4f})\")\n",
    "print(f\"  MAE:  {mae_test_s2.mean():.4f} (±{mae_test_s2.std():.4f})\")\n",
    "\n",
    "# Per-variable breakdown\n",
    "print(\"\\nPer-variable R² (Test Set):\")\n",
    "for i, var in enumerate(vars_cibles):\n",
    "    print(f\"  {var:20s}: R² = {r2_test_s2[i]:7.4f}\")\n",
    "\n",
    "print(f\"\\nOOD Performance Drop: {((r2_val_s2.mean() - r2_test_s2.mean()) / r2_val_s2.mean() * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33d12e",
   "metadata": {},
   "source": [
    "## 6. Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ad1ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO COMPARISON\n",
      "======================================================================\n",
      "\n",
      "IID vs OOD Performance Drop:\n",
      "  EB (kcal) kcal/kg brut        : IID R²= 0.989 → OOD R²= 0.940 (Δ=+0.050)\n",
      "  ED porc croissance (kcal) kcal: IID R²= 0.986 → OOD R²= 0.910 (Δ=+0.076)\n",
      "  EM porc croissance (kcal) kcal: IID R²= 0.986 → OOD R²= 0.920 (Δ=+0.067)\n",
      "  ...                           \n",
      "\n",
      "Average Performance:\n",
      "  Scenario 1 (IID Random):  Mean R² = 0.988\n",
      "  Scenario 2 (OOD Test):    Mean R² = 0.726\n",
      "  Performance Drop: 0.261\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCENARIO COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nIID vs OOD Performance Drop:\")\n",
    "for i, target in enumerate(vars_cibles):\n",
    "    r2_iid = df_results_s1.iloc[i]['R2_test']\n",
    "    r2_ood = df_results_s2.iloc[i]['R2_test']\n",
    "    drop = r2_iid - r2_ood\n",
    "    \n",
    "    if i < 3:\n",
    "        print(f\"  {target[:30]:30s}: IID R²={r2_iid:6.3f} → OOD R²={r2_ood:6.3f} (Δ={drop:+.3f})\")\n",
    "    elif i == 3:\n",
    "        print(f\"  {'...':30s}\")\n",
    "\n",
    "print(f\"\\nAverage Performance:\")\n",
    "print(f\"  Scenario 1 (IID Random):  Mean R² = {df_results_s1['R2_test'].mean():.3f}\")\n",
    "print(f\"  Scenario 2 (OOD Test):    Mean R² = {df_results_s2['R2_test'].mean():.3f}\")\n",
    "print(f\"  Performance Drop: {(df_results_s1['R2_test'].mean() - df_results_s2['R2_test'].mean()):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx4070",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
